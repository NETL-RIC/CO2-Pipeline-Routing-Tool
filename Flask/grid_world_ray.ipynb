{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import grid_world as _\n",
    "from grid_world.envs.grid_world_env_v2 import GridWorldEnv_v2\n",
    "from grid_world.envs.grid_world_env_v3 import GridWorldEnv_v3\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import imageio\n",
    "\n",
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.td3.td3 import TD3Config #TD3 only valid for continuous action spaces\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_VERSION = 'grid_world/GridWorld-v3'\n",
    "RENDER_MODE = None\n",
    "ENV_SHAPE = (25,25)\n",
    "ALGORITHM = 'PPO'\n",
    "\n",
    "env_classes = {\n",
    "    'grid_world/GridWorld-v2': GridWorldEnv_v2,\n",
    "    'grid_world/GridWorld-v3': GridWorldEnv_v3\n",
    "}\n",
    "\n",
    "env_config = {\n",
    "    'env_class': env_classes[ENV_VERSION],\n",
    "    'env_version': ENV_VERSION,\n",
    "    'render_mode':RENDER_MODE, \n",
    "    'shape':ENV_SHAPE,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 10, (4,), int64)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Register the environment with gym\n",
    "env = gym.make(env_config['env_version'])\n",
    "observation, info = env.reset()\n",
    "print(env.observation_space)\n",
    "env.close()\n",
    "print(observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def env_creator(env_config):\n",
    "\n",
    "#     env = gym.make(\n",
    "#         ENV_VERSION, \n",
    "#         env_config['render_mode'], \n",
    "#         env_config['shape']\n",
    "#         )\n",
    "\n",
    "#     return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(env_config):\n",
    "    \n",
    "    env = env_config['env_class']\n",
    "\n",
    "    return env(env_config['render_mode'], env_config['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(env_config['env_version'], env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ALGORITHM == 'DQN':\n",
    "    config = DQNConfig()\n",
    "\n",
    "if ALGORITHM == 'PPO':\n",
    "    config = PPOConfig()\n",
    "\n",
    "if ALGORITHM == 'TD3':\n",
    "    config = TD3Config()\n",
    "\n",
    "# Define the environment\n",
    "config = config.environment(env_config['env_version'])\n",
    "\n",
    "# Set the environment configuration\n",
    "config.env_config.update(env_config)\n",
    "\n",
    "# Give access to the gpu\n",
    "config = config.resources(num_gpus=1)\n",
    "\n",
    "# Set the number of training roll out workers\n",
    "config = config.rollouts(num_rollout_workers=4)\n",
    "\n",
    "# Set the max number of episode steps\n",
    "config.horizon = 30\n",
    "\n",
    "# Set the number of evaluation workers\n",
    "# config = config.evaluation(evaluation_num_workers=1)\n",
    "\n",
    "# Set framework to pytorch\n",
    "config = config.framework('torch')\n",
    "\n",
    "if ALGORITHM == 'DQN':\n",
    "    config.replay_buffer_config.update(\n",
    "        {\n",
    "            \"capacity\": 60000,\n",
    "            \"prioritized_replay_alpha\": 0.5,\n",
    "            \"prioritized_replay_beta\": 0.5,\n",
    "            \"prioritized_replay_eps\": 3e-6,\n",
    "            \"prioritized_replay\": True\n",
    "        }\n",
    "        )\n",
    "\n",
    "    # Set the training configuration\n",
    "    config = config.training(\n",
    "        noisy=True,\n",
    "        double_q=True,\n",
    "        dueling=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'StochasticSampling'}\n",
      "{'env_class': <class 'grid_world.envs.grid_world_env_v3.GridWorldEnv_v3'>, 'env_version': 'grid_world/GridWorld-v3', 'render_mode': None, 'shape': (25, 25)}\n"
     ]
    }
   ],
   "source": [
    "if ALGORITHM == 'DQN':\n",
    "    print(config.replay_buffer_config)\n",
    "print(config.exploration_config)\n",
    "print(config.env_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 14:24:59,815\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31452)\u001b[0m 2023-06-20 14:25:01,517\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31452)\u001b[0m 2023-06-20 14:25:01,518\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2023-06-20 14:25:02,498\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(rewards, lengths):\n",
    "    fig = plt.figure(1, figsize=(16, 8))\n",
    "    plt.clf()\n",
    "\n",
    "    # plt.subplots(ncols=2, figsize=(12,6))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "    ax1.set_title('Mean Rewards')\n",
    "    ax1.set_xlabel('Evaluation Interval')\n",
    "    ax1.set_ylabel('Mean Reward')\n",
    "    ax1.plot(rewards)\n",
    "\n",
    "    ax2.set_title('Mean Episode Length')\n",
    "    ax2.set_xlabel('Evaluation Interval')   \n",
    "    ax2.set_ylabel('Episode Length')\n",
    "    ax2.plot(lengths)\n",
    "    \n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "lengths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_logits._model.0.weight\n",
      "(4, 256)\n",
      "_logits._model.0.bias\n",
      "(4,)\n",
      "_hidden_layers.0._model.0.weight\n",
      "(256, 4)\n",
      "_hidden_layers.0._model.0.bias\n",
      "(256,)\n",
      "_hidden_layers.1._model.0.weight\n",
      "(256, 256)\n",
      "_hidden_layers.1._model.0.bias\n",
      "(256,)\n",
      "_value_branch_separate.0._model.0.weight\n",
      "(256, 4)\n",
      "_value_branch_separate.0._model.0.bias\n",
      "(256,)\n",
      "_value_branch_separate.1._model.0.weight\n",
      "(256, 256)\n",
      "_value_branch_separate.1._model.0.bias\n",
      "(256,)\n",
      "_value_branch._model.0.weight\n",
      "(1, 256)\n",
      "_value_branch._model.0.bias\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "policy = algo.get_policy()\n",
    "weights = policy.get_weights()\n",
    "for k,v in weights.items():\n",
    "    print(k)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 training intervals\n",
      "Completed 10 training intervals\n",
      "Completed 15 training intervals\n",
      "Completed 20 training intervals\n",
      "Completed 25 training intervals\n",
      "Completed 30 training intervals\n",
      "Completed 35 training intervals\n",
      "Completed 40 training intervals\n",
      "Completed 45 training intervals\n",
      "Completed 50 training intervals\n",
      "Completed 55 training intervals\n",
      "Completed 60 training intervals\n",
      "Completed 65 training intervals\n",
      "Completed 70 training intervals\n",
      "Completed 75 training intervals\n",
      "Completed 80 training intervals\n",
      "Completed 85 training intervals\n",
      "Completed 90 training intervals\n",
      "Completed 95 training intervals\n",
      "Completed 100 training intervals\n"
     ]
    }
   ],
   "source": [
    "TRAINING_ITERATIONS = 100\n",
    "EVAL_INTERVAL = 5\n",
    "\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "    \n",
    "    algo.train()\n",
    "\n",
    "    if (i+1) % EVAL_INTERVAL == 0:\n",
    "        print('Completed {} training intervals'.format(i+1))\n",
    "        # metrics = algo.evaluate()['evaluation']\n",
    "        # rewards.append(metrics['episode_reward_mean'])\n",
    "        # lengths.append(metrics['episode_len_mean'])\n",
    "        # plot_metrics(rewards, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment with gym\n",
    "env = gym.make(env_config['env_version'], shape=env_config['shape'])\n",
    "observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action = algo.compute_single_action(observation)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_config['env_version'], render_mode=\"human\", shape=env_config['shape'])\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(50):\n",
    "   action = algo.compute_single_action(observation)\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_config['env_version'], render_mode=\"rgb_array\", shape=env_config['shape'])\n",
    "observation, info = env.reset(seed=42)\n",
    "images = []\n",
    "images.append(env.render())\n",
    "\n",
    "for _ in range(50):\n",
    "   action = algo.compute_single_action(observation)\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   images.append(env.render())\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "imageio.mimsave('./gifs/25-25-PPO.gif', images, fps = 5)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
